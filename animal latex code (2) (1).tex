

<a href="https://colab.research.google.com/github/ipsha5/minor-project/blob/main/animal.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

\begin{verbatim}
from google.colab import drive
drive.mount('/content/drive')
\end{verbatim}

\begin{verbatim}
!sudo update-alternatives --config python3

\end{verbatim}

\begin{verbatim}
!pip install --upgrade pip
!pip install grad-cam --no-cache-dir
!pip install torch torchvision numpy matplotlib opencv-python
\end{verbatim}

\begin{verbatim}
!apt update && apt install python3-pip -y
\end{verbatim}

\begin{verbatim}
!pip --version
\end{verbatim}

\begin{verbatim}
!pip install grad-cam

\end{verbatim}

\begin{verbatim}
!pip show grad-cam

\end{verbatim}

\begin{verbatim}
!pip --version

\end{verbatim}

\begin{verbatim}
!pip install grad-cam

\end{verbatim}

\begin{verbatim}
!pip show grad-cam

\end{verbatim}

\begin{verbatim}
from pytorch_grad_cam import GradCAM
print("✅ Grad-CAM imported successfully!")

\end{verbatim}

\begin{verbatim}
!pip install torch torchvision torchaudio
!pip install grad-cam

\end{verbatim}

\begin{verbatim}
!pip show grad-cam

\end{verbatim}

\begin{verbatim}
from pytorch_grad_cam import GradCAM
print("✅ PyTorch Grad-CAM imported successfully!")

\end{verbatim}

\begin{verbatim}
import torch
import torchvision.models as models
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import preprocess_image

# Load a pretrained model
model = models.resnet18(pretrained=True)
model.eval()

# Select the last convolutional layer
target_layer = model.layer4[1].conv2

# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=[target_layer])

print("✅ Grad-CAM is ready to use!")

\end{verbatim}

\begin{verbatim}
from google.colab import files
files.upload()
\end{verbatim}

\begin{verbatim}
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
\end{verbatim}

\begin{verbatim}
!kaggle datasets download -d alessiocorrado99/animals10
!unzip animals10.zip -d /content/animal10
\end{verbatim}

\begin{verbatim}
import os
print(os.listdir("/content/animal10"))
\end{verbatim}

\section{**Image Preprocessing**}

Before training a deep learning model, images need to be preprocessed. The key steps include:

*  **Resizing & Cropping** - Ensures all images are of the same size.

*  **Normalization** - Scales pixel values to [0,1] or [-1,1] to speed up training.

*  **Handling Color Channels** - Convert images to RGB (3 channels).

*  **Data Augmentation** - Enhances dataset diversity using transformations (flips, rotations, zooms).

\begin{verbatim}
import torch
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os

\end{verbatim}

\begin{verbatim}
# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to 224x224
    transforms.ToTensor(),  # Convert images to tensors
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values
])
\end{verbatim}

\begin{verbatim}
# Set dataset path
dataset_path = "animal10/raw-img"

# Load dataset
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)

# Split dataset into training and validation
train_size = int(0.8 * len(dataset))  # 80% training
val_size = len(dataset) - train_size  # 20% validation
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print(f"Training samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}")

\end{verbatim}

\begin{verbatim}
# Function to show images
def show_images(dataloader):
    data_iter = iter(dataloader)
    images, labels = next(data_iter)

    fig, axes = plt.subplots(1, 5, figsize=(15, 5))
    for i in range(5):
        img = images[i].permute(1, 2, 0)  # Change dimension order for visualization
        img = img * 0.5 + 0.5  # De-normalize
        axes[i].imshow(img)
        axes[i].axis("off")

    plt.show()

# Display some training images
show_images(train_loader)
\end{verbatim}

\section{**Model Development**}

Deep learning models learn to classify images by extracting patterns from pixel values. There are two main approaches:

1. ***CNN from Scratch:***

*  Uses convolutional layers to extract features.

*  Needs more training data and computational power.

2.  ***Transfer Learning:***

*  Uses a pre-trained model (e.g., ResNet, VGG16) trained on large datasets like ImageNet.
*  Faster training and better accuracy.
*  We only fine-tune the last layers.

\begin{verbatim}
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
\end{verbatim}

\begin{verbatim}
# Load pre-trained ResNet18 model
model = models.resnet18(pretrained=True)

# Modify the final layer to match our number of classes
num_classes = len(train_dataset.dataset.classes)  # Number of animal categories
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

print("Model loaded successfully on:", device)

\end{verbatim}

\begin{verbatim}
# Loss function
criterion = nn.CrossEntropyLoss()

# Optimizer (Adam is usually a good choice)
optimizer = optim.Adam(model.parameters(), lr=0.001)
\end{verbatim}

\begin{verbatim}
# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5):
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Track loss and accuracy
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)

        train_loss = running_loss / len(train_loader)
        train_acc = correct / total * 100

        print(f"Epoch {epoch+1}: Loss = {train_loss:.4f}, Accuracy = {train_acc:.2f}%")

# Train the model for 5 epochs
train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)

\end{verbatim}

\begin{verbatim}
# Save trained model
torch.save(model.state_dict(), "animal10_resnet18.pth")
print("Model saved successfully!")
\end{verbatim}

\section{**Handling Class Imbalance**}

In real-world datasets, some classes may have fewer images than others. This imbalance can cause the model to:

*  Overfit to majority classes

*  Misclassify minority classes

To fix class imbalance, we use:

*  **Data Augmentation** - Artificially increase dataset size by applying transformations.

*  **Class Weighting** - Assign higher loss weights to minority classes.

*  **Resampling** - Oversample minority classes or undersample majority classes.

\begin{verbatim}
from torchvision import transforms

# Augment training images
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),  # Flip images randomly
    transforms.RandomRotation(10),  # Rotate images randomly
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color variation
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# Validation set (No augmentation)
transform_val = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
\end{verbatim}

\begin{verbatim}
# Reload dataset with augmentation
train_dataset = datasets.ImageFolder(root=dataset_path, transform=transform_train)
val_dataset = datasets.ImageFolder(root=dataset_path, transform=transform_val)

# Split dataset
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print(f"Dataset reloaded with augmentation: {len(train_dataset)} training images | {len(val_dataset)} validation images")

\end{verbatim}

\begin{verbatim}
train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)

\end{verbatim}

\section{**Explainability**}

 Using Grad-CAM Theory Neural networks are often considered black boxes because it's hard to understand why they make certain predictions. Grad-CAM (Gradient-weighted Class Activation Mapping) helps visualize the regions of an image that influence the model’s decision.

* **Bright regions (red/yellow)** - Strong influence on prediction

* **Dark regions (blue)** - Less influence

\begin{verbatim}
import torch
import torchvision.transforms as transforms
from PIL import Image
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import preprocess_image

\end{verbatim}

\begin{verbatim}
import torchvision.models as models

# Load a pretrained ResNet50 model
model = models.resnet50(pretrained=True)
model.eval()  # Set to evaluation mode

# Select the last convolutional layer
target_layer = model.layer4[-1]

\end{verbatim}

\begin{verbatim}
import os

dataset_path = "animal10/raw-img"  # Adjust the path if needed
for category in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, category)
    if os.path.isdir(class_path):
        print(f"✅ Found category: {category}, Images: {len(os.listdir(class_path))}")

\end{verbatim}

\begin{verbatim}
import cv2

image_path = "animal10/raw-img/gatto/1.jpeg"  # Change to an actual image
img = cv2.imread(image_path)
if img is None:
    print("⚠️ Image not loaded. Check file path or format.")
else:
    print("✅ Image loaded successfully!")

\end{verbatim}

\begin{verbatim}
from google.colab.patches import cv2_imshow
cv2_imshow(img)

\end{verbatim}

\begin{verbatim}
!pip install grad-cam --quiet

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision import datasets
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import preprocess_image
from google.colab.patches import cv2_imshow  # For displaying images in Colab

\end{verbatim}

\begin{verbatim}
# Load a pre-trained model (or use your trained model)
model = models.resnet18(pretrained=True)
model.eval()

# If using your trained model, load it like this:
# model.load_state_dict(torch.load("your_model.pth", map_location=torch.device('cpu')))
# model.eval()

\end{verbatim}

\begin{verbatim}
# For ResNet, the last convolutional layer is 'layer4'
target_layers = [model.layer4]

\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/gatto/6.jpeg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)

\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/mucca/OIP-3JcOkhfMrqxzvcl8MOkhMAHaEK.jpeg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/ragno/e83cb3062bf1003ed1584d05fb1d4e9fe777ead218ac104497f5c97ca5edb3bd_640.jpg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/scoiattolo/OIP-_flk9r3BRf3VxU5zWSep5QHaE8.jpeg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/pecora/e83db50a28f1073ed1584d05fb1d4e9fe777ead218ac104497f5c978a6ebb3bf_640.jpg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/cane/OIP-_fwokLb_Tpmj7UOicaeK0gHaHa.jpeg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/cavallo/OIP-0iJXerlE-7TrM413RCZm5AHaEK.jpeg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/elefante/e83db30d2df5073ed1584d05fb1d4e9fe777ead218ac104497f5c978a4efbcb0_640.jpg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# Load and preprocess image
test_image_path = "animal10/raw-img/farfalla/e83db70d28f31c22d2524518b7444f92e37fe5d404b0144390f8c770a2e9b5_640.jpg"  # Change path to an actual image
img = cv2.imread(test_image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert to tensor for Grad-CAM
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
\end{verbatim}

\begin{verbatim}
# Initialize Grad-CAM
cam = GradCAM(model=model, target_layers=target_layers)

# Generate heatmap
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Get the first (only) image's CAM

# Overlay heatmap on the original image
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\section{**Performance metrics**}

It helps us understand how well the model is classifying images. They guide us in diagnosing issues like overfitting, underfitting, and class imbalance.

For image classification, the most common evaluation metrics include:
*  **Accuracy** – Measures the percentage of correctly classified images.
* **Confusion Matrix** – Shows how well the model distinguishes between different classes.
* **F1-Score** – A balance between Precision & Recall, useful when dealing with imbalanced classes.
* **Loss & Accuracy Curves** – Helps visualize model convergence during training.

\begin{verbatim}
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Function to evaluate model performance
def evaluate_model(model, dataloader, device):
    model.eval()  # Set model to evaluation mode
    y_true = []
    y_pred = []

    with torch.no_grad():  # Disable gradient computation
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)  # Get predicted class

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

\end{verbatim}

\begin{verbatim}
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize all images
    transforms.ToTensor(),          # Convert to tensor
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize
])

\end{verbatim}

\begin{verbatim}
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),  # Flip images randomly
    transforms.RandomRotation(15),      # Rotate randomly
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Shift image slightly
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

\end{verbatim}

\begin{verbatim}
from torch.utils.data import DataLoader
import torch.nn as nn
import torch

# Calculate class weights
class_counts = [500, 800, 1200, 200, 900, 300, 400, 1000, 600, 700]  # Replace with actual class counts
class_weights = torch.tensor([1.0 / count for count in class_counts])
class_weights = class_weights.to("cuda")  # Move to GPU if needed

# Apply weighted CrossEntropyLoss
criterion = nn.CrossEntropyLoss(weight=class_weights)

\end{verbatim}

\begin{verbatim}
import torchvision.models as models

model = models.resnet50(pretrained=True)
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 10)  # 10 classes in Animal-10
model = model.to("cuda")

\end{verbatim}

\begin{verbatim}
import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR every 5 epochs

\end{verbatim}

\begin{verbatim}
import torch.nn as nn

class ImprovedCNN(nn.Module):
    def __init__(self):
        super(ImprovedCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)  # Normalize activations
        self.dropout = nn.Dropout(0.4)  # Dropout to prevent overfitting
        self.fc = nn.Linear(64 * 56 * 56, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = nn.ReLU()(x)
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

\end{verbatim}

\begin{verbatim}
from sklearn.metrics import accuracy_score, f1_score

def evaluate_model(model, test_loader, device):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="weighted")

    print(f"Accuracy: {acc:.4f}, F1-Score: {f1:.4f}")
    return acc, f1

\end{verbatim}

\begin{verbatim}
import matplotlib.pyplot as plt

def plot_training(history):
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history["train_loss"], label="Train Loss")
    plt.plot(history["val_loss"], label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Loss Curve")

    plt.subplot(1, 2, 2)
    plt.plot(history["train_acc"], label="Train Accuracy")
    plt.plot(history["val_acc"], label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.title("Accuracy Curve")

    plt.show()

\end{verbatim}

\begin{verbatim}
import torch
from sklearn.metrics import accuracy_score, f1_score

def evaluate_model(model, test_loader, device):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="weighted")

    print(f"✅ Accuracy: {acc:.4f}, F1-Score: {f1:.4f}")
    return y_true, y_pred, acc, f1

\end{verbatim}

\begin{verbatim}
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

\end{verbatim}

\begin{verbatim}
history = {"train_loss": [], "val_loss": [], "train_acc": [], "val_acc": []}

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):
    for epoch in range(epochs):
        model.train()
        train_loss, train_correct = 0.0, 0
        total_samples = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            train_correct += (preds == labels).sum().item()
            total_samples += labels.size(0)

        train_loss /= total_samples
        train_acc = train_correct / total_samples

        # Validation Phase
        model.eval()
        val_loss, val_correct = 0.0, 0
        total_samples = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item() * images.size(0)
                _, preds = torch.max(outputs, 1)
                val_correct += (preds == labels).sum().item()
                total_samples += labels.size(0)

        val_loss /= total_samples
        val_acc = val_correct / total_samples

        # Store history
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["train_acc"].append(train_acc)
        history["val_acc"].append(val_acc)

        print(f"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

    return history

\end{verbatim}

\begin{verbatim}
def plot_training_curves(history):
    plt.figure(figsize=(12, 5))

    # Loss Curve
    plt.subplot(1, 2, 1)
    plt.plot(history["train_loss"], label="Train Loss", marker="o")
    plt.plot(history["val_loss"], label="Validation Loss", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Loss Curve")

    # Accuracy Curve
    plt.subplot(1, 2, 2)
    plt.plot(history["train_acc"], label="Train Accuracy", marker="o")
    plt.plot(history["val_acc"], label="Validation Accuracy", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.title("Accuracy Curve")

    plt.show()

\end{verbatim}

\begin{verbatim}
from torchvision import datasets

# Define your dataset path
dataset_path = "animal10/raw-img"  # Change this if your dataset is in a different location

# Load dataset
dataset = datasets.ImageFolder(root=dataset_path)

# Print class names
class_names = dataset.classes
print("Class Names:", class_names)

\end{verbatim}

\begin{verbatim}
# Train the model
history = train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, device="cuda")

# Plot training curves
plot_training_curves(history)

# Evaluate the model
y_true, y_pred, acc, f1 = evaluate_model(model, test_loader, "cuda")

# Confusion Matrix
class_names = ["cane", "cavallo", "elefante", "farfalla", "gallina", "gatto", "mucca", "pecora", "ragno", "scoiattolo"]  # Update based on Animal-10 classes
plot_confusion_matrix(y_true, y_pred, class_names)

\end{verbatim}

\section{**CNN Model**}

\begin{verbatim}
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images
    transforms.RandomHorizontalFlip(p=0.5),  # Flip images horizontally (50% chance)
    transforms.RandomRotation(degrees=20),  # Rotate images randomly within ±20 degrees
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop and resize
    transforms.ToTensor(),  # Convert images to tensors
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize
])

# Load dataset with the updated transformations
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)

# Split dataset into training and validation
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print(f"Training samples: {len(train_dataset)} | Validation samples: {len(val_dataset)}")

# Show some augmented images
show_images(train_loader)

\end{verbatim}

\begin{verbatim}
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Define CNN Model
class CNNModel(nn.Module):
    def __init__(self, num_classes):
        super(CNNModel, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)

        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Fully connected layer
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))

        x = torch.flatten(x, start_dim=1)  # Flatten the feature maps
        x = F.relu(self.fc1(x))
        x = self.fc2(x)

        return x

# Get number of classes
num_classes = len(dataset.classes)
model = CNNModel(num_classes=num_classes)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(model)

\end{verbatim}

\begin{verbatim}
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

\end{verbatim}

\begin{verbatim}
num_epochs = 5  # You can increase this for better accuracy
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}")

print("Training Complete!")

\end{verbatim}

\begin{verbatim}
import cv2
import numpy as np
import torch.nn.functional as F
from torchvision.transforms.functional import normalize

# Function to apply Grad-CAM
def compute_gradcam(model, image, class_idx):
    model.eval()

    # Hook to get gradients
    gradients = []
    activations = []

    def backward_hook(module, grad_input, grad_output):
        gradients.append(grad_output[0])  # Store gradients

    def forward_hook(module, input, output):
        activations.append(output)  # Store activations

    # Register hooks on the last convolutional layer
    target_layer = model.conv3  # Last conv layer
    forward_handle = target_layer.register_forward_hook(forward_hook)
    backward_handle = target_layer.register_backward_hook(backward_hook)

    # Forward pass
    image = image.unsqueeze(0).to(device)
    output = model(image)

    # Backward pass for the specific class
    model.zero_grad()
    class_score = output[0, class_idx]
    class_score.backward()

    # Remove hooks
    forward_handle.remove()
    backward_handle.remove()

    # Compute Grad-CAM
    grads = gradients[0].cpu().detach().numpy()  # Gradient values
    activations = activations[0].cpu().detach().numpy()  # Activation maps

    weights = np.mean(grads, axis=(1, 2))  # Global average pooling
    cam = np.zeros(activations.shape[1:], dtype=np.float32)

    for i, w in enumerate(weights):
        cam += w * activations[i]

    cam = np.maximum(cam, 0)  # ReLU activation
    cam = cv2.resize(cam, (224, 224))  # Resize to original image size
    cam = cam - np.min(cam)
    cam = cam / np.max(cam)  # Normalize

    return cam

# Function to overlay Grad-CAM heatmap on image
def overlay_gradcam(image, cam):
    image = image.permute(1, 2, 0).cpu().numpy()
    image = (image - np.min(image)) / (np.max(image) - np.min(image))  # Normalize

    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    heatmap = np.float32(heatmap) / 255
    overlay = heatmap * 0.5 + image
    overlay = overlay / np.max(overlay)

    return overlay

\end{verbatim}

\begin{verbatim}
# Ensure cam has correct shape
if len(cam.shape) == 3 and cam.shape[-1] != 1:  # If shape is (H, W, C) with multiple channels
    cam = np.mean(cam, axis=-1)  # Take mean over channels to reduce to (H, W)
elif len(cam.shape) != 2:  # If it's still incorrect
    raise ValueError(f"Unexpected Grad-CAM shape: {cam.shape}")

# Normalize and convert to 8-bit
cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam) + 1e-8)  # Normalize to [0,1]
heatmap = np.uint8(255 * cam)

# Apply colormap
heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

\end{verbatim}

\begin{verbatim}
import os
print(os.listdir("animal10/"))  # List files in the directory

\end{verbatim}

\begin{verbatim}
import os

print(os.listdir("animal10/raw-img/"))  # Check files inside raw-img/

\end{verbatim}

\begin{verbatim}
# After training your CNN model, save it properly
torch.save(model.state_dict(), "animal10/custom_cnn.pth")

\end{verbatim}

\begin{verbatim}
model.load_state_dict(torch.load("animal10/custom_cnn.pth", map_location=torch.device('cpu')))

\end{verbatim}

\begin{verbatim}
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import preprocess_image
from google.colab.patches import cv2_imshow  # For displaying images in Colab

# 1️⃣ **Define Your CNN Model**
class CustomCNN(nn.Module):
    def __init__(self, num_classes=10):  # Adjust num_classes as per your dataset
        super(CustomCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 56 * 56, 128)  # Adjust based on your input size
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # Flatten
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Load trained model (modify the path if needed)
model = CustomCNN(num_classes=10)  # Adjust the number of classes
model.load_state_dict(torch.load("animal10/custom_cnn.pth", map_location=torch.device('cpu')))
model.eval()

# 2️⃣ **Select Last Convolutional Layer for Grad-CAM**
target_layers = [model.conv2]  # Adjust this based on your CNN architecture

# 3️⃣ **Load and Preprocess an Image**
image_path = "animal10/raw-img/gatto/6.jpeg"  # Change to an actual image path
img = cv2.imread(image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert image to tensor
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

# 4️⃣ **Apply Grad-CAM**
cam = GradCAM(model=model, target_layers=target_layers)
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Extract first image's CAM

# 5️⃣ **Overlay Heatmap on Original Image**
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)

\end{verbatim}

\begin{verbatim}
# 3️⃣ **Load and Preprocess an Image**
image_path = "animal10/raw-img/mucca/OIP-3JcOkhfMrqxzvcl8MOkhMAHaEK.jpeg"  # Change to an actual image path
img = cv2.imread(image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert image to tensor
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

# 4️⃣ **Apply Grad-CAM**
cam = GradCAM(model=model, target_layers=target_layers)
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Extract first image's CAM

# 5️⃣ **Overlay Heatmap on Original Image**
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# 3️⃣ **Load and Preprocess an Image**
image_path = "animal10/raw-img/farfalla/e83db70d28f31c22d2524518b7444f92e37fe5d404b0144390f8c770a2e9b5_640.jpg"  # Change to an actual image path
img = cv2.imread(image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert image to tensor
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

# 4️⃣ **Apply Grad-CAM**
cam = GradCAM(model=model, target_layers=target_layers)
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Extract first image's CAM

# 5️⃣ **Overlay Heatmap on Original Image**
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
# 3️⃣ **Load and Preprocess an Image**
image_path = "animal10/raw-img/elefante/e83db30d2df5073ed1584d05fb1d4e9fe777ead218ac104497f5c978a4efbcb0_640.jpg"  # Change to an actual image path
img = cv2.imread(image_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
img_resized = cv2.resize(img, (224, 224))  # Resize to model input size

# Convert image to tensor
input_tensor = preprocess_image(img_resized, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

# 4️⃣ **Apply Grad-CAM**
cam = GradCAM(model=model, target_layers=target_layers)
grayscale_cam = cam(input_tensor=input_tensor, targets=None)
grayscale_cam = grayscale_cam[0, :]  # Extract first image's CAM

# 5️⃣ **Overlay Heatmap on Original Image**
heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)
superimposed_img = cv2.addWeighted(img_resized, 0.5, heatmap, 0.5, 0)

# Display result
cv2_imshow(superimposed_img)
\end{verbatim}

\begin{verbatim}
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

\end{verbatim}

\begin{verbatim}
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define transformations (must match those used during training)
test_transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize to match model input size
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize
])

# Load the test dataset
test_dataset = datasets.ImageFolder(root="animal10/raw-img", transform=test_transform)

# Create DataLoader
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

# Define class labels
class_labels = test_dataset.classes  # Extract class names

\end{verbatim}

\begin{verbatim}
test_transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Ensure this matches your training size
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Ensure same normalization
])

\end{verbatim}

\begin{verbatim}
def forward(self, x):
    x = self.features(x)
    print("Shape before flattening:", x.shape)  # Debugging
    x = x.view(x.size(0), -1)  # Flatten
    print("Shape after flattening:", x.shape)  # Debugging
    x = self.classifier(x)
    return x

\end{verbatim}

\begin{verbatim}
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

\end{verbatim}

\begin{verbatim}
# Define transformations for training and testing datasets
train_transform = transforms.Compose([
    transforms.Resize((128, 128)),  # Resize images
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

test_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

\end{verbatim}

\begin{verbatim}
# Set dataset path
data_path = "/content/animal10/raw-img/"

# Load datasets
train_dataset = datasets.ImageFolder(root=f"{data_path}", transform=train_transform)
test_dataset = datasets.ImageFolder(root=f"{data_path}", transform=test_transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Get class names
class_names = train_dataset.classes
print("Classes:", class_names)

\end{verbatim}

\begin{verbatim}
import torch
import torch.nn as nn
import torch.nn.functional as F

class ImprovedCNN(nn.Module):
    def __init__(self):
        super(ImprovedCNN, self).__init__()

        # Convolutional Layers
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(128)

        # Pooling Layer to reduce feature size
        self.pool = nn.AdaptiveAvgPool2d((7, 7))  # Reduces spatial size to (7,7)

        # Fully Connected Layers
        self.fc1 = nn.Linear(128 * 7 * 7, 256)  # Adjusted based on the final feature map size
        self.fc2 = nn.Linear(256, 10)  # Output layer for 10 classes

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))  # Convolution + BatchNorm + ReLU
        x = F.relu(self.bn2(self.conv2(x)))  # Convolution + BatchNorm + ReLU
        x = self.pool(x)  # Reduce size to (7,7)

        x = x.view(x.size(0), -1)  # Flatten the feature map
        x = F.relu(self.fc1(x))  # Fully connected layer
        x = self.fc2(x)  # Output layer

        return x


\end{verbatim}

\begin{verbatim}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ImprovedCNN().to(device)

\end{verbatim}

\begin{verbatim}
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

\end{verbatim}

\begin{verbatim}
def train_model(model, train_loader, criterion, optimizer, epochs, device):
    model.train()

    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct / total

        print(f"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}")

    return model


\end{verbatim}

\begin{verbatim}
def evaluate_model(model, test_loader, device):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)

            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    return y_true, y_pred

\end{verbatim}

\begin{verbatim}
def plot_training_curves(history):
    plt.figure(figsize=(12, 5))

    # Loss Curve
    plt.subplot(1, 2, 1)
    plt.plot(history["train_loss"], label="Train Loss", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.title("Loss Curve")

    # Accuracy Curve
    plt.subplot(1, 2, 2)
    plt.plot(history["train_acc"], label="Train Accuracy", marker="o")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.title("Accuracy Curve")

    plt.show()

plot_training_curves(history)

\end{verbatim}

\begin{verbatim}
from sklearn.metrics import accuracy_score, f1_score

def evaluate_model(model, dataloader, device):
    model.eval()
    y_true, y_pred = [], []

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(preds.cpu().numpy())

    # Calculate Accuracy & F1-score
    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average="weighted")  # Weighted F1-score

    print(f"✅ Accuracy: {acc:.4f}, F1-Score: {f1:.4f}")  # Print results

    return y_true, y_pred, acc, f1  # Return values


\end{verbatim}

\begin{verbatim}
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred, class_names):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()

# ✅ Run model evaluation first
y_true, y_pred = evaluate_model(model, test_loader, device)

# ✅ Now plot confusion matrix
class_names = ['class1', 'class2', ..., 'class10']  # Update with your actual class labels
plot_confusion_matrix(y_true, y_pred, class_names)

\end{verbatim}

ResNet is better than a regular CNN because it uses shortcut connections to avoid vanishing gradients, allowing it to train deeper networks with higher accuracy.



\end{document}
